{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 강화학습 튜토리얼\n",
    "## 인공 신경망으로 Pong을 학습시키자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 원문 : http://karpathy.github.io/2016/05/31/rl/\n",
    "* 번역 : http://keunwoochoi.blogspot.kr/2016/06/andrej-karpathy.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 패키지 임포트 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cPickle as pickle\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 하이퍼파라미터 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 히든 레이어의 뉴런 200개\n",
    "* 배치 사이즈 10개씩\n",
    "* 러닝 레이트는 0.0001\n",
    "* discount factor 0.99 <현 상태의 보상 대비 다음 스텝의 보상의 가치의 비율>\n",
    "* RMSProp의 decay_rate 0.99\n",
    "* resume (True)면 저장된 체크포인트에서 모델 불러온다.\n",
    "* render (True)면 매 step의 agent와 environment를 렌더링한다.(화면 출력)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "H = 200 # number of hidden layer neurons\n",
    "batch_size = 10 # every how many episodes to do a param update?\n",
    "learning_rate = 1e-4\n",
    "gamma = 0.99 # discount factor for reward\n",
    "decay_rate = 0.99 # decay factor for RMSProp leaky sum of grad^2\n",
    "resume = True # resume from previous checkpoint?\n",
    "render = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 초기화 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://ishuca.tistory.com/entry/CS231n-Neural-Networks-Part-3-Learning-and-Evaluation-%ED%95%9C%EA%B5%AD%EC%96%B4-%EB%B2%88%EC%97%AD\n",
    "\n",
    "체크포인트가 존재하면 그 모델을 픽클로 불러오고 아니라면 2 layer를 가진 인공신경망의 weights를 저장할 model dictionary와 각 weights들의 gradient를 업데이트에 사용하기 위해 저장할 공간인 grad_buffer, rmsprop optimizer의 cache값을 저장할 rmsprop_cache를 dictionary 형태로 초기화한다. (RMSprop에 관한 설명은 위 링크 참고)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.19072044,  1.4974702 , -0.14425513],\n",
       "       [ 1.24759543, -0.34712257, -0.90154552]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randn(2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros_like([1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model initialization\n",
    "D = 80 * 80 # input dimensionality: 80x80 grid\n",
    "if resume:\n",
    "    model = pickle.load(open('save.p', 'rb'))\n",
    "else:\n",
    "    model = {}\n",
    "    model['W1'] = np.random.randn(H,D) / np.sqrt(D) # \"Xavier\" initialization\n",
    "    model['W2'] = np.random.randn(H) / np.sqrt(H)\n",
    "    \n",
    "grad_buffer = { k : np.zeros_like(v) for k,v in model.iteritems() } # update buffers that add up gradients over a batch\n",
    "rmsprop_cache = { k : np.zeros_like(v) for k,v in model.iteritems() } # rmsprop memory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input 전처리 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "연산량을 줄이기 위해 210x160x3의 프레임을 80x80(6400)의 1차원 벡터로 전처리한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prepro(I):\n",
    "    \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n",
    "    I = I[35:195] # crop\n",
    "    I = I[::2,::2,0] # downsample by factor of 2\n",
    "    I[I == 144] = 0 # erase background (background type 1)\n",
    "    I[I == 109] = 0 # erase background (background type 2)\n",
    "    I[I != 0] = 1 # everything else (paddles, ball) just set to 1\n",
    "    return I.astype(np.float).ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Output layer를 위한 Sigmoid 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x): \n",
    "    return 1.0 / (1.0 + np.exp(-x)) # sigmoid \"squashing\" function to interval [0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Network "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우선 정책망을 우리의 선수(player) 혹은 요원 (agent)라고 정의하자. 정책망은 현재 게임의 상태가 어떤지를 입력으로 받고 이를 토대로 우리가 어떤 행동을 취해야하는지 (UP or DOWN; 막대기를 위로 올릴지 아래로 내릴지)를 판단한다. 데이터(화면)과 레이블(행동) 사이를 인공 신경망으로 이어주자. 여기에서는 층이 2개인 인공 신경망을 사용할 것이다. 여기에서 입력은 100,800 (210 * 160 * 3)개의 노드가 되고 최종적으로 UP이 될 확률이 출력이 될 것이다. 이 구성은 흔하게 쓰이는 Stochastic 정책이다. (어떤 선택을 내릴지의 확률을 구하는 경우 여기에 해당한다.) 학습 과정에서 우리는 이 확률 분포를 토대로 어떤 행동을 취할 것인지를 샘플링을 한다. 예를 들어 70%의 확률로 UP인 상황이라면 p(UP):p(DOWN) = 7:3인 동전을 튀겨서 나온 결과를 사용한다. 이렇게 하는 이유는 뒤에서 설명하겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def policy_forward(x):\n",
    "    # 2 layer의 NN\n",
    "    # Output layer는 sigmoid(binary)\n",
    "    h = np.dot(model['W1'], x)\n",
    "    h[h<0] = 0 # ReLU nonlinearity\n",
    "    logp = np.dot(model['W2'], h)\n",
    "    p = sigmoid(logp)\n",
    "    return p, h \n",
    "# Agent가 올라갈 확률(action 2), hidden state 리턴"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discount_rewards "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0a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\n9mi/u/rRRRcB6IqLtUYFFFFID//Z\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image('./image/pong_tuto_1.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discount_rewards(r):\n",
    "    \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    for t in reversed(xrange(0, r.size)):\n",
    "        if r[t] != 0: running_add = 0 # reset the sum, since this was a game boundary (pong specific!)\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "np.outer(a,b) // 벡터의 외적"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def policy_backward(eph, epdlogp):\n",
    "    \"\"\" backward pass. (eph is array of intermediate hidden states) \"\"\"\n",
    "    dW2 = np.dot(eph.T, epdlogp).ravel()\n",
    "    dh = np.outer(epdlogp, model['W2'])\n",
    "    dh[eph <= 0] = 0 # backpro prelu\n",
    "    dW1 = np.dot(dh.T, epx)\n",
    "    return {'W1':dW1, 'W2':dW2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment 셋팅 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-10-30 22:56:50,518] Making new env: Pong-v0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Pong-v0\")\n",
    "observation = env.reset()\n",
    "prev_x = None # used in computing the difference frame\n",
    "xs,hs,dlogps,drs = [],[],[],[]\n",
    "running_reward = None\n",
    "reward_sum = 0 # 전체 보상을 더할 카운터\n",
    "episode_number = 0 # 에피소드 카운터(몇 판했는지)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(6)\n",
      "Box(210, 160, 3)\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space)\n",
    "print(env.observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(210, 160, 3)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6400,)\n"
     ]
    }
   ],
   "source": [
    "test = prepro(observation)\n",
    "print test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resetting env. episode reward total was -20.000000. running mean: -20.000000\n",
      "ep 1: game finished, reward: -1.000000\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.010000\n",
      "ep 2: game finished, reward: -1.000000\n",
      "resetting env. episode reward total was -19.000000. running mean: -19.999900\n",
      "ep 3: game finished, reward: -1.000000\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.009901\n",
      "ep 4: game finished, reward: -1.000000\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.019802\n",
      "ep 5: game finished, reward: -1.000000\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.019604\n",
      "ep 6: game finished, reward: -1.000000\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.019408\n",
      "ep 7: game finished, reward: -1.000000\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.029214\n",
      "ep 8: game finished, reward: -1.000000\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.018922\n",
      "ep 9: game finished, reward: -1.000000\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.028732\n",
      "ep 10: game finished, reward: -1.000000\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.038445\n",
      "ep 11: game finished, reward: -1.000000\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.048061\n",
      "ep 12: game finished, reward: -1.000000\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.057580\n",
      "ep 13: game finished, reward: -1.000000\n",
      "resetting env. episode reward total was -18.000000. running mean: -20.037004\n",
      "ep 14: game finished, reward: -1.000000\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.036634\n",
      "ep 15: game finished, reward: -1.000000\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.036268\n",
      "ep 16: game finished, reward: -1.000000\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.035905\n",
      "ep 17: game finished, reward: -1.000000\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.045546\n",
      "ep 18: game finished, reward: -1.000000\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.055091\n",
      "ep 19: game finished, reward: -1.000000\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.044540\n",
      "ep 20: game finished, reward: -1.000000\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.044094\n",
      "ep 21: game finished, reward: -1.000000\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.053653\n",
      "ep 22: game finished, reward: -1.000000\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.063117\n",
      "ep 23: game finished, reward: -1.000000\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.072486\n",
      "ep 24: game finished, reward: -1.000000\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.081761\n",
      "ep 25: game finished, reward: -1.000000\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.080943\n",
      "ep 26: game finished, reward: -1.000000\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.080134\n",
      "ep 27: game finished, reward: -1.000000\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.089333\n",
      "ep 28: game finished, reward: -1.000000\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.098439\n",
      "ep 29: game finished, reward: -1.000000\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.097455\n",
      "ep 30: game finished, reward: -1.000000\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.096480\n",
      "ep 31: game finished, reward: -1.000000\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.095515\n",
      "ep 32: game finished, reward: -1.000000\n",
      "resetting env. episode reward total was -18.000000. running mean: -20.074560\n",
      "ep 33: game finished, reward: -1.000000\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.073815\n",
      "ep 34: game finished, reward: -1.000000\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.063077\n",
      "ep 35: game finished, reward: -1.000000\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.062446\n",
      "ep 36: game finished, reward: -1.000000\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.051821\n",
      "ep 37: game finished, reward: -1.000000\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.041303\n",
      "ep 38: game finished, reward: -1.000000\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.040890\n",
      "ep 39: game finished, reward: -1.000000\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.050481\n",
      "ep 40: game finished, reward: -1.000000\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.049976\n",
      "ep 41: game finished, reward: -1.000000\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.049477\n",
      "ep 42: game finished, reward: -1.000000\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.048982\n",
      "ep 43: game finished, reward: -1.000000\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.038492\n",
      "ep 44: game finished, reward: -1.000000\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.028107\n",
      "ep 45: game finished, reward: -1.000000\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.037826\n",
      "ep 46: game finished, reward: -1.000000\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.027448\n",
      "ep 47: game finished, reward: -1.000000\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.037173\n",
      "ep 48: game finished, reward: -1.000000\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.046802\n",
      "ep 49: game finished, reward: -1.000000\n",
      "resetting env. episode reward total was -18.000000. running mean: -20.026334\n",
      "ep 50: game finished, reward: -1.000000\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.036070\n",
      "ep 51: game finished, reward: -1.000000\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.035710\n",
      "ep 52: game finished, reward: -1.000000\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.045352\n",
      "ep 53: game finished, reward: -1.000000\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.044899\n",
      "ep 54: game finished, reward: -1.000000\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.054450\n",
      "ep 55: game finished, reward: -1.000000\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.043905\n",
      "ep 56: game finished, reward: -1.000000\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.053466\n",
      "ep 57: game finished, reward: -1.000000\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.052932\n",
      "ep 58: game finished, reward: -1.000000\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.062402\n",
      "ep 59: game finished, reward: -1.000000\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.071778\n",
      "ep 60: game finished, reward: -1.000000\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.081061\n",
      "ep 61: game finished, reward: -1.000000\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.090250\n",
      "ep 62: game finished, reward: -1.000000\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.099347\n",
      "ep 63: game finished, reward: -1.000000\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.108354\n",
      "ep 64: game finished, reward: -1.000000\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.117270\n",
      "ep 65: game finished, reward: -1.000000\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.126098\n",
      "ep 66: game finished, reward: -1.000000\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.124837\n",
      "ep 67: game finished, reward: -1.000000\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.123588\n",
      "ep 68: game finished, reward: -1.000000\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.122353\n",
      "ep 69: game finished, reward: -1.000000\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.121129\n",
      "ep 70: game finished, reward: -1.000000\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.119918\n",
      "ep 71: game finished, reward: -1.000000\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.128719\n",
      "ep 72: game finished, reward: -1.000000\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.137431\n",
      "ep 73: game finished, reward: -1.000000\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.146057\n",
      "ep 74: game finished, reward: -1.000000\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.154596\n",
      "ep 75: game finished, reward: -1.000000\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.143050\n",
      "ep 76: game finished, reward: -1.000000\n",
      "resetting env. episode reward total was -18.000000. running mean: -20.121620\n",
      "ep 77: game finished, reward: -1.000000\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.130404\n",
      "ep 78: game finished, reward: -1.000000\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.129100\n",
      "ep 79: game finished, reward: -1.000000\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.117809\n",
      "ep 80: game finished, reward: -1.000000\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.126631\n",
      "ep 81: game finished, reward: -1.000000\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.115364\n",
      "ep 82: game finished, reward: -1.000000\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.114211\n",
      "ep 83: game finished, reward: -1.000000\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.123069\n",
      "ep 84: game finished, reward: -1.000000\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.111838\n",
      "ep 85: game finished, reward: -1.000000\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.120720\n",
      "ep 86: game finished, reward: -1.000000\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.119512\n",
      "ep 87: game finished, reward: -1.000000\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.128317\n",
      "ep 88: game finished, reward: -1.000000\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.127034\n",
      "ep 89: game finished, reward: -1.000000\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.135764\n",
      "ep 90: game finished, reward: -1.000000\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.144406\n",
      "ep 91: game finished, reward: -1.000000\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.152962\n",
      "ep 92: game finished, reward: -1.000000\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.151432\n",
      "ep 93: game finished, reward: -1.000000\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.149918\n",
      "ep 94: game finished, reward: -1.000000\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.138419\n",
      "ep 95: game finished, reward: -1.000000\n",
      "resetting env. episode reward total was -21.000000. running mean: -20.147035\n",
      "ep 96: game finished, reward: -1.000000\n",
      "resetting env. episode reward total was -20.000000. running mean: -20.145564\n",
      "ep 97: game finished, reward: -1.000000\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.134109\n",
      "ep 98: game finished, reward: -1.000000\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.122768\n",
      "ep 99: game finished, reward: -1.000000\n",
      "resetting env. episode reward total was -19.000000. running mean: -20.111540\n",
      "ep 100: game finished, reward: -1.000000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-3857ee792654>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# 전처리 후 전 프레임과 현 프레임과의 차이를 인풋으로 사용한다!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/gym/core.pyc\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    190\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnsupportedMode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unsupported rendering mode: {}. (Supported modes for {}: {})'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/gym/envs/atari/atari_env.pyc\u001b[0m in \u001b[0;36m_render\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSimpleImageViewer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_action_meanings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/gym/envs/classic_control/rendering.pyc\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(self, arr)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswitch_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/pyglet/image/__init__.pyc\u001b[0m in \u001b[0;36mblit\u001b[0;34m(self, x, y, z, width, height)\u001b[0m\n\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mblit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 892\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_texture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mblit_to_texture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minternalformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/pyglet/image/__init__.pyc\u001b[0m in \u001b[0;36mget_texture\u001b[0;34m(self, rectangle, force_rectangle)\u001b[0m\n\u001b[1;32m    816\u001b[0m             self._current_texture = self.create_texture(Texture, \n\u001b[1;32m    817\u001b[0m                                                         \u001b[0mrectangle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 818\u001b[0;31m                                                         force_rectangle)\n\u001b[0m\u001b[1;32m    819\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_texture\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/pyglet/image/__init__.pyc\u001b[0m in \u001b[0;36mcreate_texture\u001b[0;34m(self, cls, rectangle, force_rectangle)\u001b[0m\n\u001b[1;32m    801\u001b[0m         \u001b[0minternalformat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_internalformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m         texture = cls.create(self.width, self.height, internalformat, \n\u001b[0;32m--> 803\u001b[0;31m                              rectangle, force_rectangle)\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manchor_x\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manchor_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m             \u001b[0mtexture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manchor_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manchor_x\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/pyglet/image/__init__.pyc\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, width, height, internalformat, rectangle, force_rectangle, min_filter, mag_filter)\u001b[0m\n\u001b[1;32m   1512\u001b[0m                      \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m                      \u001b[0mGL_RGBA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGL_UNSIGNED_BYTE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1514\u001b[0;31m                      blank)\n\u001b[0m\u001b[1;32m   1515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1516\u001b[0m         \u001b[0mtexture\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexture_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexture_height\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/pyglet/gl/lib.pyc\u001b[0m in \u001b[0;36merrcheck\u001b[0;34m(result, func, arguments)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0merrcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marguments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_debug_gl_trace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    if render: env.render()\n",
    "    \n",
    "    env.render()\n",
    "    \n",
    "    # 전처리 후 전 프레임과 현 프레임과의 차이를 인풋으로 사용한다! \n",
    "    cur_x = prepro(observation)\n",
    "    x = cur_x - prev_x if prev_x is not None else np.zeros(D) \n",
    "    prev_x = cur_x\n",
    "\n",
    "    # 정책망을 foward로 흘려보내 action2(Up)할 확률을 구하고 그 확률로부터 action을 샘플링한다.\n",
    "    aprob, h = policy_forward(x)\n",
    "    action = 2 if np.random.uniform() < aprob else 3 # 동전 던지기! 2이면 up, 3이면 down인건가?\n",
    "\n",
    "    # 나중에 backprop을 위해 필요한 정보들을 저장한다. \n",
    "    xs.append(x) # observation\n",
    "    hs.append(h) # hidden state\n",
    "    \n",
    "    # 이 부분은 policy gradient 관련 내용이라 모르겠음.. 나중에 차차 알아봅시다!\n",
    "    y = 1 if action == 2 else 0 # a \"fake label\"\n",
    "    dlogps.append(y - aprob) \n",
    "    # grad that encourages the action that was taken to be taken (see http://cs231n.github.io/neural-networks-2/#losses if confused)\n",
    "\n",
    "    # 다음 스텝으로 진행한다. 현 step에서 action을 취한다.\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    reward_sum += reward\n",
    "\n",
    "    drs.append(reward) # record reward (has to be done after we call step() to get reward for previous action)\n",
    "\n",
    "    if done: # 게임 한판이 끝나면 \n",
    "        episode_number += 1\n",
    "        # vstack을 사용하여 그 한판(에피소드)에 있었던 inputs, hidden states, action gradients, rewards를 모두 합친다!\n",
    "        # https://docs.scipy.org/doc/numpy/reference/generated/numpy.vstack.html\n",
    "        epx = np.vstack(xs)\n",
    "        eph = np.vstack(hs)\n",
    "        epdlogp = np.vstack(dlogps)\n",
    "        epr = np.vstack(drs)\n",
    "        xs,hs,dlogps,drs = [],[],[],[] # 다음판의 저장소로 사용하기 위해 다시 초기화\n",
    "        \n",
    "        # 각 state의 보상의 discount rewards를 구한다\n",
    "        discounted_epr = discount_rewards(epr)\n",
    "        # standardize the rewards to be unit normal (helps control the gradient estimator variance)\n",
    "        discounted_epr -= np.mean(discounted_epr)\n",
    "        discounted_epr /= np.std(discounted_epr)\n",
    "        \n",
    "        # 이 부분은 policy gradient 관련 내용이라 모르겠음.. 나중에 차차 알아봅시다!\n",
    "        epdlogp *= discounted_epr # modulate the gradient with advantage (PG magic happens right here.)\n",
    "        \n",
    "        # backprop을 통해 grad를 계산한다\n",
    "        grad = policy_backward(eph, epdlogp)\n",
    "        for k in model: grad_buffer[k] += grad[k] # accumulate grad over batch\n",
    "        \n",
    "        # episodes가 배치사이즈 만큼 쌓이면 RMSPROP를 통해 parameter update를 수행한다!\n",
    "        if episode_number % batch_size == 0:\n",
    "            for k,v in model.iteritems():\n",
    "                g = grad_buffer[k] # gradient\n",
    "                rmsprop_cache[k] = decay_rate * rmsprop_cache[k] + (1 - decay_rate) * g**2\n",
    "                model[k] += learning_rate * g / (np.sqrt(rmsprop_cache[k]) + 1e-5)\n",
    "                grad_buffer[k] = np.zeros_like(v) # reset batch gradient buffer\n",
    "\n",
    "        # boring book-keeping\n",
    "        # reward_sum과 running_reward의 차이가 무엇이지?\n",
    "        running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
    "        print 'resetting env. episode reward total was %f. running mean: %f' % (reward_sum, running_reward)\n",
    "        if episode_number % 100 == 0: pickle.dump(model, open('save.p', 'wb'))\n",
    "        reward_sum = 0\n",
    "        observation = env.reset() # reset env\n",
    "        prev_x = None\n",
    "\n",
    "        if reward != 0: # Pong has either +1 or -1 reward exactly when game ends.\n",
    "            print ('ep %d: game finished, reward: %f' % (episode_number, reward)) + ('' if reward == -1 else ' !!!!!!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
