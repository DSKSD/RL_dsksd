{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import gym\n",
    "import itertools\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "\n",
    "if \"../\" not in sys.path:\n",
    "  sys.path.append(\"../\")\n",
    "\n",
    "from lib import plotting\n",
    "from collections import deque, namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-02-23 10:53:09,467] Making new env: Breakout-v0\n"
     ]
    }
   ],
   "source": [
    "env = gym.envs.make(\"Breakout-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Atari Actions: 0 (noop), 1 (fire), 2 (left) and 3 (right) are valid actions\n",
    "VALID_ACTIONS = [0, 1, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class StateProcessor():\n",
    "    \"\"\"\n",
    "    raw 픽셀을 리사이즈해준다\n",
    "    Processes a raw Atari iamges. Resizes it and converts it to grayscale.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # Build the Tensorflow graph\n",
    "        with tf.variable_scope(\"state_processor\"):\n",
    "            self.input_state = tf.placeholder(shape=[210, 160, 3], dtype=tf.uint8)\n",
    "            self.output = tf.image.rgb_to_grayscale(self.input_state) # RGB -> Gray Scale (210,160)\n",
    "            self.output = tf.image.crop_to_bounding_box(self.output, 34, 0, 160, 160) #  CROP\n",
    "            self.output = tf.image.resize_images(\n",
    "                self.output,[84,84], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR) # (84,84)로 리사이즈\n",
    "            self.output = tf.squeeze(self.output)\n",
    "\n",
    "    def process(self, sess, state):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sess: A Tensorflow session object\n",
    "            state: A [210, 160, 3] Atari RGB State\n",
    "\n",
    "        Returns:\n",
    "            A processed [84, 84, 1] state representing grayscale values.\n",
    "        \"\"\"\n",
    "        return sess.run(self.output, { self.input_state: state })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Code 분석 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(210, 160, 3)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test1 = env.reset()\n",
    "test1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(210, 160, 1)\n",
      "(160, 160, 1)\n",
      "(84, 84, 1)\n",
      "(84, 84)\n",
      "(84, 84, 4)\n"
     ]
    }
   ],
   "source": [
    "input1 =  tf.placeholder(shape=[210, 160, 3], dtype=tf.uint8)\n",
    "gray = tf.image.rgb_to_grayscale(input1)\n",
    "crop = tf.image.crop_to_bounding_box(gray, 34, 0, 160, 160)\n",
    "poutput = tf.image.resize_images(\n",
    "                    crop,[84,84], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "\n",
    "output = tf.squeeze(poutput)\n",
    "\n",
    "#tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    g = sess.run(gray, feed_dict = {input1 : test1})\n",
    "    print(g.shape)  # gray scale\n",
    "    c = sess.run(crop,{input1:test1})\n",
    "    print(c.shape) # crop\n",
    "    p = sess.run(poutput,{input1:test1})\n",
    "    print(p.shape)\n",
    "    o = sess.run(output,{input1:test1})\n",
    "    print(o.shape)\n",
    "    observation = np.stack([o] * 4, axis=2)\n",
    "    print(observation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1, 2, 1, 2, 1, 2, 1, 2],\n",
       "        [2, 3, 2, 3, 2, 3, 2, 3],\n",
       "        [3, 4, 3, 4, 3, 4, 3, 4]]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.stack([[[1,2,3]],[[2,3,4]]]*4,axis=2) # 왜 4배로 뿔리는거지?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tf.squeeze(input, axis=None, name=None, squeeze_dims=None) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removes dimensions of size 1 from the shape of a tensor.\n",
    "\n",
    "중간중간 (불필요한) 1차원들을 지워준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Estimator():\n",
    "    \"\"\"Q-Value Estimator neural network.\n",
    "    \n",
    "    Q-value를 근사하는 network (Q-value는 그 state에서 action을 했을 때 예상되는 value)\n",
    "    Q-Network(행동 e-greedy), Target Nework(타겟 TD(0)해보고 greedy하게 가져가는) \n",
    "    \n",
    "    This network is used for both the Q-Network and the Target Network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, scope=\"estimator\", summaries_dir=None):\n",
    "        self.scope = scope\n",
    "        # Writes Tensorboard summaries to disk\n",
    "        self.summary_writer = None\n",
    "        with tf.variable_scope(scope):\n",
    "            # Build the graph\n",
    "            self._build_model()\n",
    "            if summaries_dir:\n",
    "                summary_dir = os.path.join(summaries_dir, \"summaries_{}\".format(scope))\n",
    "                if not os.path.exists(summary_dir):\n",
    "                    os.makedirs(summary_dir)\n",
    "                self.summary_writer =  writer = tf.summary.FileWriter(summary_dir, sess.graph)\n",
    "\n",
    "    def _build_model(self):\n",
    "        \"\"\"\n",
    "        Builds the Tensorflow graph.\n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "        # Input을 위한 placeholder 정의 \n",
    "        \n",
    "        # Our input are 4 RGB frames of shape 160, 160 each\n",
    "        self.X_pl = tf.placeholder(shape=[None, 84, 84, 4], dtype=tf.uint8, name=\"X\")\n",
    "        # The TD target value (그 state에서 greedy하게 action을 취했을 때 얻어지는 값)\n",
    "        self.y_pl = tf.placeholder(shape=[None], dtype=tf.float32, name=\"y\")\n",
    "        # 어떤 액션이 선택되었는지에 대한 값\n",
    "        self.actions_pl = tf.placeholder(shape=[None], dtype=tf.int32, name=\"actions\")\n",
    "        \n",
    "        # 인풋을 픽셀에 맞게 normalize 해주는 부분인듯!\n",
    "        X = tf.to_float(self.X_pl) / 255.0\n",
    "        batch_size = tf.shape(self.X_pl)[0] # batch_size는 None?? 상관없다??\n",
    "        \n",
    "        \n",
    "        # 3 계층의 convolutional layers\n",
    "        # input, num_outputs, kernel_size, stride , activation  // padding은 same? , pooling은? \n",
    "        conv1 = tf.contrib.layers.conv2d(\n",
    "            X, 32, 8, 4, activation_fn=tf.nn.relu) \n",
    "        conv2 = tf.contrib.layers.conv2d(\n",
    "            conv1, 64, 4, 2, activation_fn=tf.nn.relu)\n",
    "        conv3 = tf.contrib.layers.conv2d(\n",
    "            conv2, 64, 3, 1, activation_fn=tf.nn.relu)\n",
    "\n",
    "        # Fully connected layers (히든 레이어 하나인듯)\n",
    "        flattened = tf.contrib.layers.flatten(conv3) # conv layer 부분을 fc에 넣기 위해 펼친다 // 벡터로\n",
    "        fc1 = tf.contrib.layers.fully_connected(flattened, 512)\n",
    "        self.predictions = tf.contrib.layers.fully_connected(fc1, len(VALID_ACTIONS)) # 액션 4개로~\n",
    "        \n",
    "        # 여긴 뭐지? 좀 더 봐야 할듯\n",
    "        # Get the predictions for the chosen actions only\n",
    "        gather_indices = tf.range(batch_size) * tf.shape(self.predictions)[1] + self.actions_pl # batch의 각 instance마다 action의 index?\n",
    "        self.action_predictions = tf.gather(tf.reshape(self.predictions, [-1]), gather_indices)\n",
    "\n",
    "        # Target Error (Loss 정의)\n",
    "        self.losses = tf.squared_difference(self.y_pl, self.action_predictions)\n",
    "        self.loss = tf.reduce_mean(self.losses) # MSE\n",
    "\n",
    "        # Optimizer Parameters from original paper\n",
    "        self.optimizer = tf.train.RMSPropOptimizer(0.00025, 0.99, 0.0, 1e-6)\n",
    "        self.train_op = self.optimizer.minimize(self.loss, global_step=tf.contrib.framework.get_global_step())\n",
    "        \n",
    "        # Summaries for Tensorboard\n",
    "        with tf.name_scope(\"summaries\"):\n",
    "            tf.summary.scalar(\"loss\", self.loss)\n",
    "            tf.summary.histogram(\"loss_hist\", self.losses)\n",
    "            tf.summary.histogram(\"q_values_hist\", self.predictions)\n",
    "            tf.summary.scalar(\"max_q_value\", tf.reduce_max(self.predictions))\n",
    "            \n",
    "            self.summaries = tf.summary.merge_all()\n",
    "\n",
    "\n",
    "    def predict(self, sess, s):\n",
    "        \"\"\"\n",
    "        Predicts action values.\n",
    "\n",
    "        Args:\n",
    "          sess: Tensorflow session\n",
    "          s: State input of shape [batch_size, 4, 160, 160, 3]\n",
    "\n",
    "        Returns:\n",
    "          Tensor of shape [batch_size, NUM_VALID_ACTIONS] containing the estimated \n",
    "          action values.\n",
    "        \"\"\"\n",
    "        return sess.run(self.predictions, { self.X_pl: s }) # s가 들어왔을 때 선택되는 action(predictions)\n",
    "\n",
    "    def update(self, sess, s, a, y):\n",
    "        \"\"\"\n",
    "        Updates the estimator towards the given targets.\n",
    "\n",
    "        Args:\n",
    "          sess: Tensorflow session object\n",
    "          s: State input of shape [batch_size, 4, 160, 160, 3]\n",
    "          a: Chosen actions of shape [batch_size]\n",
    "          y: Targets of shape [batch_size]\n",
    "\n",
    "        Returns:\n",
    "          The calculated loss on the batch.\n",
    "        \"\"\"\n",
    "        feed_dict = { self.X_pl: s, self.y_pl: y, self.actions_pl: a }\n",
    "        summaries, global_step, _, loss = sess.run(\n",
    "            [self.summaries, tf.contrib.framework.get_global_step(), self.train_op, self.loss],\n",
    "            feed_dict)\n",
    "        if self.summary_writer:\n",
    "            self.summary_writer.add_summary(summaries, global_step)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tf.range(start, limit=None, delta=1, dtype=None, name='range')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'start' is 3\n",
    "'limit' is 18\n",
    "'delta' is 3<br>\n",
    "tf.range(start, limit, delta) ==> [3, 6, 9, 12, 15]\n",
    "\n",
    "'start' is 3\n",
    "'limit' is 1\n",
    "'delta' is -0.5<br>\n",
    "tf.range(start, limit, delta) ==> [3, 2.5, 2, 1.5]\n",
    "\n",
    "'limit' is 5 <br>\n",
    "tf.range(limit) ==> [0, 1, 2, 3, 4]|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tf.gather(params, indices, validate_indices=None, name=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients_impl.py:91: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84, 84)\n",
      "(84, 84, 4)\n",
      "(84, 84)\n",
      "(84, 84, 1)\n",
      "(84, 84, 4)\n",
      "[[ 0.02584658  0.          0.02007666  0.04457932]\n",
      " [ 0.02584658  0.          0.02007666  0.04457932]]\n",
      "99.5552\n"
     ]
    }
   ],
   "source": [
    "# For Testing....\n",
    "\n",
    "tf.reset_default_graph()\n",
    "global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "\n",
    "e = Estimator(scope=\"test\")\n",
    "sp = StateProcessor()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Example observation batch\n",
    "    observation = env.reset()\n",
    "    \n",
    "    observation_p = sp.process(sess, observation)\n",
    "    observation = np.stack([observation_p] * 4, axis=2)\n",
    "    observations = np.array([observation] * 2)\n",
    "    \n",
    "    print(observation_p.shape)\n",
    "    print(observation.shape)\n",
    "  #  print(observations.shape)\n",
    "    \n",
    "\n",
    "    action = np.random.choice(np.arange(4))\n",
    "    next_state, reward, done, _ = env.step(VALID_ACTIONS[action])\n",
    "    next_state = sp.process(sess, next_state)\n",
    "    print(next_state.shape)\n",
    "    print(np.expand_dims(next_state, 2).shape)\n",
    "    next_state = np.append(observation[:,:,1:], np.expand_dims(next_state, 2), axis=2)\n",
    "    \n",
    "    print(next_state.shape)\n",
    "    \n",
    "    \n",
    "    # Test Prediction\n",
    "    print(e.predict(sess, observations))\n",
    "\n",
    "    # Test training step\n",
    "    y = np.array([10.0, 10.0])\n",
    "    a = np.array([1, 3])\n",
    "    print(e.update(sess, observations, a, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def copy_model_parameters(sess, estimator1, estimator2):\n",
    "    \"\"\"\n",
    "    Copies the model parameters of one estimator to another.\n",
    "\n",
    "    Args:\n",
    "      sess: Tensorflow session instance\n",
    "      estimator1: Estimator to copy the paramters from\n",
    "      estimator2: Estimator to copy the parameters to\n",
    "    \"\"\"\n",
    "    e1_params = [t for t in tf.trainable_variables() if t.name.startswith(estimator1.scope)]\n",
    "    e1_params = sorted(e1_params, key=lambda v: v.name)\n",
    "    e2_params = [t for t in tf.trainable_variables() if t.name.startswith(estimator2.scope)]\n",
    "    e2_params = sorted(e2_params, key=lambda v: v.name)\n",
    "\n",
    "    update_ops = []\n",
    "    for e1_v, e2_v in zip(e1_params, e2_params):\n",
    "        op = e2_v.assign(e1_v)\n",
    "        update_ops.append(op)\n",
    "\n",
    "    sess.run(update_ops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tf.trainable_variables() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returns all variables created with trainable=True.\n",
    "\n",
    "When passed trainable=True, the Variable() constructor automatically adds new variables to the graph collection GraphKeys.TRAINABLE_VARIABLES. This convenience function returns the contents of that collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def make_epsilon_greedy_policy(estimator, nA):\n",
    "    \"\"\"\n",
    "    Creates an epsilon-greedy policy based on a given Q-function approximator and epsilon.\n",
    "\n",
    "    Args:\n",
    "        estimator: An estimator that returns q values for a given state\n",
    "        nA: Number of actions in the environment.\n",
    "\n",
    "    Returns:\n",
    "        A function that takes the (sess, observation, epsilon) as an argument and returns\n",
    "        the probabilities for each action in the form of a numpy array of length nA.\n",
    "\n",
    "    \"\"\"\n",
    "    def policy_fn(sess, observation, epsilon):\n",
    "        A = np.ones(nA, dtype=float) * epsilon / nA\n",
    "        q_values = estimator.predict(sess, np.expand_dims(observation, 0))[0]\n",
    "        best_action = np.argmax(q_values)\n",
    "        A[best_action] += (1.0 - epsilon)\n",
    "        return A\n",
    "    return policy_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def deep_q_learning(sess,\n",
    "                    env,\n",
    "                    q_estimator,\n",
    "                    target_estimator,\n",
    "                    state_processor,\n",
    "                    num_episodes,\n",
    "                    experiment_dir,\n",
    "                    replay_memory_size=500000,\n",
    "                    replay_memory_init_size=50000,\n",
    "                    update_target_estimator_every=10000,\n",
    "                    discount_factor=0.99,\n",
    "                    epsilon_start=1.0,\n",
    "                    epsilon_end=0.1,\n",
    "                    epsilon_decay_steps=500000,\n",
    "                    batch_size=32,\n",
    "                    record_video_every=50):\n",
    "    \"\"\"\n",
    "    Q-Learning algorithm for fff-policy TD control using Function Approximation.\n",
    "    Finds the optimal greedy policy while following an epsilon-greedy policy.\n",
    "\n",
    "    Args:\n",
    "        sess: Tensorflow Session object\n",
    "        env: OpenAI environment\n",
    "        q_estimator: Estimator object used for the q values\n",
    "        target_estimator: Estimator object used for the targets\n",
    "        state_processor: A StateProcessor object\n",
    "        num_episodes: Number of episodes to run for\n",
    "        experiment_dir: Directory to save Tensorflow summaries in\n",
    "        replay_memory_size: Size of the replay memory\n",
    "        replay_memory_init_size: Number of random experiences to sampel when initializing \n",
    "          the reply memory.\n",
    "        update_target_estimator_every: Copy parameters from the Q estimator to the \n",
    "          target estimator every N steps\n",
    "        discount_factor: Lambda time discount factor\n",
    "        epsilon_start: Chance to sample a random action when taking an action.\n",
    "          Epsilon is decayed over time and this is the start value\n",
    "        epsilon_end: The final minimum value of epsilon after decaying is done\n",
    "        epsilon_decay_steps: Number of steps to decay epsilon over\n",
    "        batch_size: Size of batches to sample from the replay memory\n",
    "        record_video_every: Record a video every N episodes\n",
    "\n",
    "    Returns:\n",
    "        An EpisodeStats object with two numpy arrays for episode_lengths and episode_rewards.\n",
    "    \"\"\"\n",
    "    # 상태 전이를 표현할 튜플\n",
    "    Transition = namedtuple(\"Transition\", [\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "\n",
    "    # The replay memory\n",
    "    replay_memory = []\n",
    "\n",
    "    # Keeps track of useful statistics\n",
    "    stats = plotting.EpisodeStats(\n",
    "        episode_lengths=np.zeros(num_episodes),\n",
    "        episode_rewards=np.zeros(num_episodes))\n",
    "\n",
    "    # Create directories for checkpoints and summaries\n",
    "    checkpoint_dir = os.path.join(experiment_dir, \"ckps\")\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, \"model\")\n",
    "    monitor_path = os.path.join(experiment_dir, \"monitor\")\n",
    "\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    if not os.path.exists(monitor_path):\n",
    "        os.makedirs(monitor_path)\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    # Load a previous checkpoint if we find one\n",
    "    latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "    if latest_checkpoint:\n",
    "        print(\"Loading model checkpoint {}...\\n\".format(latest_checkpoint))\n",
    "        saver.restore(sess, latest_checkpoint)\n",
    "    \n",
    "    # Get the current time step\n",
    "    total_t = sess.run(tf.contrib.framework.get_global_step())\n",
    "\n",
    "    # epsilon이 시간에 따라 decay되도록 term을 가지고 배열로 만들어 놓기 (시작값, 끝값, 간격)\n",
    "    epsilons = np.linspace(epsilon_start, epsilon_end, epsilon_decay_steps)\n",
    "\n",
    "    # 행동 policy < e-greedy >\n",
    "    policy = make_epsilon_greedy_policy(\n",
    "        q_estimator,\n",
    "        len(VALID_ACTIONS))\n",
    "\n",
    "    # 초기 experience들을 replay memory가 꽉 찰 때까지 넣어준다.\n",
    "    print(\"Populating replay memory...\")\n",
    "    state = env.reset() # 210*160*3\n",
    "    state = state_processor.process(sess, state) # 84*84\n",
    "    state = np.stack([state] * 4, axis=2) # 84*84*4\n",
    "    for i in range(replay_memory_init_size):\n",
    "        action_probs = policy(sess, state, epsilons[min(total_t, epsilon_decay_steps-1)])\n",
    "        action = np.random.choice(np.arange(len(action_probs)), p=action_probs) # 액션 선택\n",
    "        next_state, reward, done, _ = env.step(VALID_ACTIONS[action]) # 다음 스텝 해보기\n",
    "        next_state = state_processor.process(sess, next_state)\n",
    "        next_state = np.append(state[:,:,1:], np.expand_dims(next_state, 2), axis=2) # 84*84*4\n",
    "        replay_memory.append(Transition(state, action, reward, next_state, done))\n",
    "        if done:\n",
    "            state = env.reset()\n",
    "            state = state_processor.process(sess, state)\n",
    "            state = np.stack([state] * 4, axis=2)\n",
    "        else:\n",
    "            state = next_state\n",
    "\n",
    "    # Record videos\n",
    "   # gym.wrappers.Monitor(env, monitor_path)\n",
    "#     env.monitor.start(monitor_path,\n",
    "#                       resume=True,\n",
    "#                       video_callable=lambda count: count % record_video_every == 0)\n",
    "\n",
    "    for i_episode in range(num_episodes):\n",
    "\n",
    "        # Save the current checkpoint\n",
    "        saver.save(tf.get_default_session(), checkpoint_path)\n",
    "\n",
    "        # Reset the environment\n",
    "        state = env.reset()\n",
    "        state = state_processor.process(sess, state)\n",
    "        state = np.stack([state] * 4, axis=2)\n",
    "        loss = None\n",
    "\n",
    "        # One step in the environment\n",
    "        for t in itertools.count():\n",
    "\n",
    "            # Epsilon for this time step\n",
    "            epsilon = epsilons[min(total_t, epsilon_decay_steps-1)]\n",
    "\n",
    "            # Add epsilon to Tensorboard\n",
    "            episode_summary = tf.Summary()\n",
    "            episode_summary.value.add(simple_value=epsilon, tag=\"epsilon\")\n",
    "            q_estimator.summary_writer.add_summary(episode_summary, total_t)\n",
    "\n",
    "            # Maybe update the target estimator\n",
    "            if total_t % update_target_estimator_every == 0:\n",
    "                copy_model_parameters(sess, q_estimator, target_estimator) # 일정 step마다 target_network update\n",
    "                print(\"\\nCopied model parameters to target network.\")\n",
    "            \n",
    "            # Print out which step we're on, useful for debugging.\n",
    "            sys.stdout.write(\"\\rStep {} ({}) @ Episode {}/{}, loss: {}\".format(\n",
    "                    t, total_t, i_episode + 1, num_episodes, loss))\n",
    "            sys.stdout.flush()\n",
    "\n",
    "            # Take a step\n",
    "            action_probs = policy(sess, state, epsilon)\n",
    "            action = np.random.choice(np.arange(len(action_probs)), p=action_probs) # 액션 선택\n",
    "            next_state, reward, done, _ = env.step(VALID_ACTIONS[action]) #한번 가보고\n",
    "            next_state = state_processor.process(sess, next_state)\n",
    "            next_state = np.append(state[:,:,1:], np.expand_dims(next_state, 2), axis=2) # 84*84*4\n",
    "\n",
    "            # 꽉 차면 오래된 기억부터 팝\n",
    "            if len(replay_memory) == replay_memory_size:\n",
    "                replay_memory.pop(0)\n",
    "\n",
    "            # 새 기억 집어넣고\n",
    "            replay_memory.append(Transition(state, action, reward, next_state, done))   \n",
    "\n",
    "            # Update statistics\n",
    "            stats.episode_rewards[i_episode] += reward\n",
    "            stats.episode_lengths[i_episode] = t\n",
    "\n",
    "            # replay_memory에서 배치사이즈만큼을 샘플링한다!\n",
    "            samples = random.sample(replay_memory, batch_size)\n",
    "            states_batch, action_batch, reward_batch, next_states_batch, done_batch = map(np.array, zip(*samples)) \n",
    "            # 각 step 별로 [state,action,reward, next_state,done]으로 묶여있던거를 np.array로 타입 바꾸고 병렬적으로 쫙 풀어서 나눠담기\n",
    "\n",
    "            # Calculate q values and targets\n",
    "            q_values_next = target_estimator.predict(sess, next_states_batch)\n",
    "            targets_batch = reward_batch + np.invert(done_batch).astype(np.float32) * discount_factor * np.amax(q_values_next, axis=1)\n",
    "            # 타겟 value!! target_estimator로 예측한... greedy target!!!\n",
    "            \n",
    "            \n",
    "            # Perform gradient descent update\n",
    "            states_batch = np.array(states_batch) # 형변환 왜 또 하는거지..?\n",
    "            # Loss를 reduce_mean을 통해 scalar로 받아오고, Optimizing까지 해준다. \n",
    "            loss = q_estimator.update(sess, states_batch, action_batch, targets_batch)\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            state = next_state\n",
    "            total_t += 1 # global step??\n",
    "\n",
    "        # Add summaries to tensorboard\n",
    "        episode_summary = tf.Summary()\n",
    "        episode_summary.value.add(simple_value=stats.episode_rewards[i_episode], node_name=\"episode_reward\", tag=\"episode_reward\")\n",
    "        episode_summary.value.add(simple_value=stats.episode_lengths[i_episode], node_name=\"episode_length\", tag=\"episode_length\")\n",
    "        q_estimator.summary_writer.add_summary(episode_summary, total_t)\n",
    "        q_estimator.summary_writer.flush()\n",
    "\n",
    "        yield total_t, plotting.EpisodeStats(\n",
    "            episode_lengths=stats.episode_lengths[:i_episode+1],\n",
    "            episode_rewards=stats.episode_rewards[:i_episode+1])\n",
    "\n",
    "#     env.monitor.close()\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients_impl.py:91: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating replay memory...\n",
      "\n",
      "Copied model parameters to target network.\n",
      "Step 175 (175) @ Episode 1/10000, loss: 0.00033073188387788833\n",
      "Episode Reward: 0.0\n",
      "Step 160 (335) @ Episode 2/10000, loss: 0.03188493102788925017\n",
      "Episode Reward: 0.0\n",
      "Step 210 (545) @ Episode 3/10000, loss: 0.00020879003568552434\n",
      "Episode Reward: 1.0\n",
      "Step 347 (892) @ Episode 4/10000, loss: 0.00025085199740715325\n",
      "Episode Reward: 4.0\n",
      "Step 264 (1156) @ Episode 5/10000, loss: 0.00017177494009956717\n",
      "Episode Reward: 1.0\n",
      "Step 431 (1587) @ Episode 6/10000, loss: 5.192850949242711e-056\n",
      "Episode Reward: 4.0\n",
      "Step 214 (1801) @ Episode 7/10000, loss: 0.00031539093470200896\n",
      "Episode Reward: 1.0\n",
      "Step 205 (2006) @ Episode 8/10000, loss: 8.935783989727497e-055\n",
      "Episode Reward: 1.0\n",
      "Step 316 (2322) @ Episode 9/10000, loss: 8.901790715754032e-057\n",
      "Episode Reward: 3.0\n",
      "Step 163 (2485) @ Episode 10/10000, loss: 0.00014565957826562226\n",
      "Episode Reward: 0.0\n",
      "Step 302 (2787) @ Episode 11/10000, loss: 7.081223884597421e-059\n",
      "Episode Reward: 2.0\n",
      "Step 180 (2967) @ Episode 12/10000, loss: 0.00011556794925127178\n",
      "Episode Reward: 0.0\n",
      "Step 278 (3245) @ Episode 13/10000, loss: 7.598145748488605e-056\n",
      "Episode Reward: 2.0\n",
      "Step 277 (3522) @ Episode 14/10000, loss: 0.03236855193972587665\n",
      "Episode Reward: 2.0\n",
      "Step 176 (3698) @ Episode 15/10000, loss: 0.00021873181685805328\n",
      "Episode Reward: 0.0\n",
      "Step 168 (3866) @ Episode 16/10000, loss: 9.808983304537833e-055\n",
      "Episode Reward: 0.0\n",
      "Step 209 (4075) @ Episode 17/10000, loss: 0.03246475011110306113\n",
      "Episode Reward: 1.0\n",
      "Step 177 (4252) @ Episode 18/10000, loss: 0.03074227832257747787\n",
      "Episode Reward: 0.0\n",
      "Step 161 (4413) @ Episode 19/10000, loss: 0.00010625105642247945\n",
      "Episode Reward: 0.0\n",
      "Step 244 (4657) @ Episode 20/10000, loss: 0.03132364526391029456\n",
      "Episode Reward: 1.0\n",
      "Step 400 (5057) @ Episode 21/10000, loss: 7.820357859600335e-054\n",
      "Episode Reward: 4.0\n",
      "Step 349 (5406) @ Episode 22/10000, loss: 0.00012979484745301306\n",
      "Episode Reward: 3.0\n",
      "Step 339 (5745) @ Episode 23/10000, loss: 0.00021585886133834728\n",
      "Episode Reward: 3.0\n",
      "Step 242 (5987) @ Episode 24/10000, loss: 0.00013115895853843548\n",
      "Episode Reward: 1.0\n",
      "Step 175 (6162) @ Episode 25/10000, loss: 7.612626359332353e-054\n",
      "Episode Reward: 0.0\n",
      "Step 368 (6530) @ Episode 26/10000, loss: 0.03095172345638275665\n",
      "Episode Reward: 4.0\n",
      "Step 172 (6702) @ Episode 27/10000, loss: 0.00011936832743231207\n",
      "Episode Reward: 0.0\n",
      "Step 170 (6872) @ Episode 28/10000, loss: 0.00011026499851141125\n",
      "Episode Reward: 0.0\n",
      "Step 228 (7100) @ Episode 29/10000, loss: 0.03108898550271988555\n",
      "Episode Reward: 1.0\n",
      "Step 304 (7404) @ Episode 30/10000, loss: 9.301715181209147e-054\n",
      "Episode Reward: 2.0\n",
      "Step 167 (7571) @ Episode 31/10000, loss: 0.00025000789901241667\n",
      "Episode Reward: 0.0\n",
      "Step 193 (7764) @ Episode 32/10000, loss: 0.00012402705033309758\n",
      "Episode Reward: 0.0\n",
      "Step 251 (8015) @ Episode 33/10000, loss: 7.618257950525731e-057\n",
      "Episode Reward: 2.0\n",
      "Step 187 (8202) @ Episode 34/10000, loss: 8.874095510691404e-056\n",
      "Episode Reward: 0.0\n",
      "Step 358 (8560) @ Episode 35/10000, loss: 0.00010072298755403608\n",
      "Episode Reward: 3.0\n",
      "Step 199 (8759) @ Episode 36/10000, loss: 5.651167157338932e-059\n",
      "Episode Reward: 0.0\n",
      "Step 286 (9045) @ Episode 37/10000, loss: 9.175704326480627e-058\n",
      "Episode Reward: 2.0\n",
      "Step 319 (9364) @ Episode 38/10000, loss: 5.1579729188233614e-05\n",
      "Episode Reward: 3.0\n",
      "Step 317 (9681) @ Episode 39/10000, loss: 7.903696678113192e-056\n",
      "Episode Reward: 3.0\n",
      "Step 173 (9854) @ Episode 40/10000, loss: 0.00047199541586451235\n",
      "Episode Reward: 0.0\n",
      "Step 145 (9999) @ Episode 41/10000, loss: 0.00014519655087497085\n",
      "Copied model parameters to target network.\n",
      "Step 358 (10212) @ Episode 41/10000, loss: 0.00027397216763347397\n",
      "Episode Reward: 3.0\n",
      "Step 231 (10443) @ Episode 42/10000, loss: 0.00018533888214733452\n",
      "Episode Reward: 1.0\n",
      "Step 225 (10668) @ Episode 43/10000, loss: 0.00020528360619209707\n",
      "Episode Reward: 1.0\n",
      "Step 311 (10979) @ Episode 44/10000, loss: 0.00029165635351091623\n",
      "Episode Reward: 2.0\n",
      "Step 171 (11150) @ Episode 45/10000, loss: 0.00026715829153545257\n",
      "Episode Reward: 0.0\n",
      "Step 291 (11441) @ Episode 46/10000, loss: 0.00013464665971696377\n",
      "Episode Reward: 2.0\n",
      "Step 160 (11601) @ Episode 47/10000, loss: 0.00029249922954477377\n",
      "Episode Reward: 0.0\n",
      "Step 204 (11805) @ Episode 48/10000, loss: 0.00022309628548100595\n",
      "Episode Reward: 1.0\n",
      "Step 175 (11980) @ Episode 49/10000, loss: 0.00036305753747001296\n",
      "Episode Reward: 0.0\n",
      "Step 400 (12380) @ Episode 50/10000, loss: 0.00030049070483073598\n",
      "Episode Reward: 4.0\n",
      "Step 182 (12562) @ Episode 51/10000, loss: 0.00032786105293780565\n",
      "Episode Reward: 0.0\n",
      "Step 241 (12803) @ Episode 52/10000, loss: 0.03339935839176178665\n",
      "Episode Reward: 1.0\n",
      "Step 298 (13101) @ Episode 53/10000, loss: 8.52638331707567e-0507\n",
      "Episode Reward: 2.0\n",
      "Step 176 (13277) @ Episode 54/10000, loss: 0.00018730023293755949\n",
      "Episode Reward: 0.0\n",
      "Step 178 (13455) @ Episode 55/10000, loss: 0.00025893902056850493\n",
      "Episode Reward: 0.0\n",
      "Step 268 (13723) @ Episode 56/10000, loss: 0.00016616069478914142\n",
      "Episode Reward: 2.0\n",
      "Step 175 (13898) @ Episode 57/10000, loss: 0.00026566023007035255\n",
      "Episode Reward: 0.0\n",
      "Step 338 (14236) @ Episode 58/10000, loss: 0.03324680402874946645\n",
      "Episode Reward: 3.0\n",
      "Step 169 (14405) @ Episode 59/10000, loss: 0.00036032969364896417\n",
      "Episode Reward: 0.0\n",
      "Step 382 (14787) @ Episode 60/10000, loss: 0.00019916694145649672\n",
      "Episode Reward: 4.0\n",
      "Step 297 (15084) @ Episode 61/10000, loss: 0.00015379162505269052\n",
      "Episode Reward: 3.0\n",
      "Step 194 (15278) @ Episode 62/10000, loss: 0.00029443408129736785\n",
      "Episode Reward: 0.0\n",
      "Step 240 (15518) @ Episode 63/10000, loss: 0.00018193959840573377\n",
      "Episode Reward: 1.0\n",
      "Step 224 (15742) @ Episode 64/10000, loss: 0.00015491763770114636\n",
      "Episode Reward: 1.0\n",
      "Step 175 (15917) @ Episode 65/10000, loss: 0.00027298647910356526\n",
      "Episode Reward: 0.0\n",
      "Step 161 (16078) @ Episode 66/10000, loss: 0.00027829353348352015\n",
      "Episode Reward: 0.0\n",
      "Step 180 (16258) @ Episode 67/10000, loss: 0.00038975683855824175\n",
      "Episode Reward: 0.0\n",
      "Step 213 (16471) @ Episode 68/10000, loss: 0.00036427759914658964\n",
      "Episode Reward: 1.0\n",
      "Step 215 (16686) @ Episode 69/10000, loss: 0.03329503163695335463\n",
      "Episode Reward: 1.0\n",
      "Step 173 (16859) @ Episode 70/10000, loss: 0.00034670764580368996\n",
      "Episode Reward: 0.0\n",
      "Step 236 (17095) @ Episode 71/10000, loss: 0.06393525004386902013\n",
      "Episode Reward: 1.0\n",
      "Step 278 (17373) @ Episode 72/10000, loss: 0.00025807396741583943\n",
      "Episode Reward: 2.0\n",
      "Step 209 (17582) @ Episode 73/10000, loss: 0.00026469514705240726\n",
      "Episode Reward: 1.0\n",
      "Step 212 (17794) @ Episode 74/10000, loss: 0.00013957242481410503\n",
      "Episode Reward: 1.0\n",
      "Step 234 (18028) @ Episode 75/10000, loss: 0.00028677657246589663\n",
      "Episode Reward: 1.0\n",
      "Step 301 (18329) @ Episode 76/10000, loss: 0.00026459182845428586\n",
      "Episode Reward: 2.0\n",
      "Step 181 (18510) @ Episode 77/10000, loss: 0.00041433982551097872\n",
      "Episode Reward: 0.0\n",
      "Step 312 (18822) @ Episode 78/10000, loss: 0.00020074851636309177\n",
      "Episode Reward: 2.0\n",
      "Step 359 (19181) @ Episode 79/10000, loss: 0.00030873133800923824\n",
      "Episode Reward: 3.0\n",
      "Step 163 (19344) @ Episode 80/10000, loss: 0.00025312538491562015\n",
      "Episode Reward: 0.0\n",
      "Step 237 (19581) @ Episode 81/10000, loss: 0.00025632677716203034\n",
      "Episode Reward: 1.0\n",
      "Step 227 (19808) @ Episode 82/10000, loss: 0.00040365802124142647\n",
      "Episode Reward: 1.0\n",
      "Step 191 (19999) @ Episode 83/10000, loss: 0.00030805816641077473\n",
      "Copied model parameters to target network.\n",
      "Step 309 (20117) @ Episode 83/10000, loss: 0.00024573496193625033\n",
      "Episode Reward: 2.0\n",
      "Step 437 (20554) @ Episode 84/10000, loss: 0.00015401754353661092\n",
      "Episode Reward: 5.0\n",
      "Step 247 (20801) @ Episode 85/10000, loss: 0.00019520029309205715\n",
      "Episode Reward: 2.0\n",
      "Step 329 (21130) @ Episode 86/10000, loss: 0.00035878719063475735\n",
      "Episode Reward: 3.0\n",
      "Step 255 (21385) @ Episode 87/10000, loss: 0.00024860116536729047\n",
      "Episode Reward: 2.0\n",
      "Step 168 (21553) @ Episode 88/10000, loss: 0.00049478170694783336\n",
      "Episode Reward: 0.0\n",
      "Step 308 (21861) @ Episode 89/10000, loss: 0.00055312720360234384\n",
      "Episode Reward: 3.0\n",
      "Step 265 (22126) @ Episode 90/10000, loss: 0.00033903325675055385\n",
      "Episode Reward: 2.0\n",
      "Step 176 (22302) @ Episode 91/10000, loss: 0.00039078900590538989\n",
      "Episode Reward: 0.0\n",
      "Step 188 (22490) @ Episode 92/10000, loss: 0.00044727549538947645\n",
      "Episode Reward: 0.0\n",
      "Step 243 (22733) @ Episode 93/10000, loss: 0.03159094601869583264\n",
      "Episode Reward: 1.0\n",
      "Step 265 (22998) @ Episode 94/10000, loss: 0.00056131859309971336\n",
      "Episode Reward: 2.0\n",
      "Step 174 (23172) @ Episode 95/10000, loss: 0.00050072540761902936\n",
      "Episode Reward: 0.0\n",
      "Step 311 (23483) @ Episode 96/10000, loss: 0.00045384303666651254\n",
      "Episode Reward: 3.0\n",
      "Step 176 (23659) @ Episode 97/10000, loss: 0.00037027121288701894\n",
      "Episode Reward: 0.0\n",
      "Step 284 (23943) @ Episode 98/10000, loss: 0.03024846501648426395\n",
      "Episode Reward: 2.0\n",
      "Step 206 (24149) @ Episode 99/10000, loss: 0.00051964877638965856\n",
      "Episode Reward: 1.0\n",
      "Step 232 (24381) @ Episode 100/10000, loss: 0.00023822915682103485\n",
      "Episode Reward: 1.0\n",
      "Step 174 (24555) @ Episode 101/10000, loss: 0.06125060841441154564\n",
      "Episode Reward: 0.0\n",
      "Step 214 (24769) @ Episode 102/10000, loss: 0.00047654734225943685\n",
      "Episode Reward: 1.0\n",
      "Step 242 (25011) @ Episode 103/10000, loss: 0.00048562296433374286\n",
      "Episode Reward: 1.0\n",
      "Step 174 (25185) @ Episode 104/10000, loss: 0.00028857472352683544\n",
      "Episode Reward: 0.0\n",
      "Step 299 (25484) @ Episode 105/10000, loss: 0.00038551070610992617\n",
      "Episode Reward: 2.0\n",
      "Step 382 (25866) @ Episode 106/10000, loss: 0.00048229060485027735\n",
      "Episode Reward: 4.0\n",
      "Step 252 (26118) @ Episode 107/10000, loss: 0.03099644556641578795\n",
      "Episode Reward: 2.0\n",
      "Step 297 (26415) @ Episode 108/10000, loss: 0.00027980015147477394\n",
      "Episode Reward: 2.0\n",
      "Step 211 (26626) @ Episode 109/10000, loss: 0.00023862092348281294\n",
      "Episode Reward: 1.0\n",
      "Step 177 (26803) @ Episode 110/10000, loss: 0.00047807503142394125\n",
      "Episode Reward: 0.0\n",
      "Step 170 (26973) @ Episode 111/10000, loss: 0.03453383222222328697\n",
      "Episode Reward: 0.0\n",
      "Step 248 (27221) @ Episode 112/10000, loss: 0.00052575027802959086\n",
      "Episode Reward: 1.0\n",
      "Step 303 (27524) @ Episode 113/10000, loss: 0.00038724887417629365\n",
      "Episode Reward: 3.0\n",
      "Step 388 (27912) @ Episode 114/10000, loss: 0.00027355266502127058\n",
      "Episode Reward: 4.0\n",
      "Step 256 (28168) @ Episode 115/10000, loss: 0.00057475478388369083\n",
      "Episode Reward: 2.0\n",
      "Step 286 (28454) @ Episode 116/10000, loss: 0.12898734211921692516\n",
      "Episode Reward: 2.0\n",
      "Step 269 (28723) @ Episode 117/10000, loss: 0.00048810098087415124\n",
      "Episode Reward: 1.0\n",
      "Step 349 (29072) @ Episode 118/10000, loss: 0.00043210590956732636\n",
      "Episode Reward: 3.0\n",
      "Step 177 (29249) @ Episode 119/10000, loss: 0.00071662856498733165\n",
      "Episode Reward: 0.0\n",
      "Step 178 (29427) @ Episode 120/10000, loss: 0.00046826474135741596\n",
      "Episode Reward: 0.0\n",
      "Step 233 (29660) @ Episode 121/10000, loss: 0.03054208867251873675\n",
      "Episode Reward: 1.0\n",
      "Step 244 (29904) @ Episode 122/10000, loss: 0.00056736980332061658\n",
      "Episode Reward: 1.0\n",
      "Step 95 (29999) @ Episode 123/10000, loss: 0.00064089370425790557\n",
      "Copied model parameters to target network.\n",
      "Step 210 (30114) @ Episode 123/10000, loss: 0.00053568050498142846\n",
      "Episode Reward: 1.0\n",
      "Step 183 (30297) @ Episode 124/10000, loss: 0.00030448869802057743\n",
      "Episode Reward: 0.0\n",
      "Step 274 (30571) @ Episode 125/10000, loss: 0.03150441125035286556\n",
      "Episode Reward: 2.0\n",
      "Step 315 (30886) @ Episode 126/10000, loss: 0.00063840410439297565\n",
      "Episode Reward: 2.0\n",
      "Step 173 (31059) @ Episode 127/10000, loss: 0.00039064095471985646\n",
      "Episode Reward: 0.0\n",
      "Step 264 (31323) @ Episode 128/10000, loss: 0.00045809629955329694\n",
      "Episode Reward: 2.0\n",
      "Step 205 (31528) @ Episode 129/10000, loss: 0.00068378960713744164\n",
      "Episode Reward: 1.0\n",
      "Step 253 (31781) @ Episode 130/10000, loss: 0.00040506187360733753\n",
      "Episode Reward: 1.0\n",
      "Step 219 (32000) @ Episode 131/10000, loss: 0.00041221984429284934\n",
      "Episode Reward: 1.0\n",
      "Step 233 (32233) @ Episode 132/10000, loss: 0.00067113945260643964\n",
      "Episode Reward: 1.0\n",
      "Step 168 (32401) @ Episode 133/10000, loss: 0.00068064715014770633\n",
      "Episode Reward: 0.0\n",
      "Step 170 (32571) @ Episode 134/10000, loss: 0.00043069222010672096\n",
      "Episode Reward: 0.0\n",
      "Step 312 (32883) @ Episode 135/10000, loss: 0.03158376738429069545\n",
      "Episode Reward: 3.0\n",
      "Step 175 (33058) @ Episode 136/10000, loss: 0.03080856241285801075\n",
      "Episode Reward: 0.0\n",
      "Step 261 (33319) @ Episode 137/10000, loss: 0.00038790079997852445\n",
      "Episode Reward: 2.0\n",
      "Step 179 (33498) @ Episode 138/10000, loss: 0.00052551669068634515\n",
      "Episode Reward: 0.0\n",
      "Step 179 (33677) @ Episode 139/10000, loss: 0.00066470779711380645\n",
      "Episode Reward: 0.0\n",
      "Step 246 (33923) @ Episode 140/10000, loss: 0.00054523407015949497\n",
      "Episode Reward: 1.0\n",
      "Step 316 (34239) @ Episode 141/10000, loss: 0.00034490053076297045\n",
      "Episode Reward: 2.0\n",
      "Step 215 (34454) @ Episode 142/10000, loss: 0.00018654688028618693\n",
      "Episode Reward: 1.0\n",
      "Step 329 (34783) @ Episode 143/10000, loss: 0.00049563968786969787\n",
      "Episode Reward: 3.0\n",
      "Step 188 (34971) @ Episode 144/10000, loss: 0.00045735226012766367\n",
      "Episode Reward: 0.0\n",
      "Step 167 (35138) @ Episode 145/10000, loss: 0.00028348961495794356\n",
      "Episode Reward: 0.0\n",
      "Step 335 (35473) @ Episode 146/10000, loss: 0.03069235384464264825\n",
      "Episode Reward: 3.0\n",
      "Step 317 (35790) @ Episode 147/10000, loss: 0.00052824337035417563\n",
      "Episode Reward: 3.0\n",
      "Step 181 (35971) @ Episode 148/10000, loss: 0.00044988642912358046\n",
      "Episode Reward: 0.0\n",
      "Step 238 (36209) @ Episode 149/10000, loss: 0.00039801871753297746\n",
      "Episode Reward: 1.0\n",
      "Step 338 (36547) @ Episode 150/10000, loss: 0.00039599070441909134\n",
      "Episode Reward: 3.0\n",
      "Step 186 (36733) @ Episode 151/10000, loss: 0.00035207075416110456\n",
      "Episode Reward: 0.0\n",
      "Step 172 (36905) @ Episode 152/10000, loss: 0.00037039426388219327\n",
      "Episode Reward: 0.0\n",
      "Step 258 (37163) @ Episode 153/10000, loss: 0.00039540612488053746\n",
      "Episode Reward: 1.0\n",
      "Step 415 (37578) @ Episode 154/10000, loss: 0.00033740498474799097\n",
      "Episode Reward: 4.0\n",
      "Step 174 (37752) @ Episode 155/10000, loss: 0.00039468103204853833\n",
      "Episode Reward: 0.0\n",
      "Step 414 (38166) @ Episode 156/10000, loss: 0.00038322480395436287\n",
      "Episode Reward: 4.0\n",
      "Step 271 (38437) @ Episode 157/10000, loss: 0.00047834205906838185\n",
      "Episode Reward: 2.0\n",
      "Step 198 (38635) @ Episode 158/10000, loss: 0.00058506906498223543\n",
      "Episode Reward: 0.0\n",
      "Step 170 (38805) @ Episode 159/10000, loss: 0.00029289559461176395\n",
      "Episode Reward: 0.0\n",
      "Step 444 (39249) @ Episode 160/10000, loss: 0.00045470742043107753\n",
      "Episode Reward: 5.0\n",
      "Step 283 (39532) @ Episode 161/10000, loss: 0.00038600232801400125\n",
      "Episode Reward: 1.0\n",
      "Step 194 (39726) @ Episode 162/10000, loss: 0.00080063915811479096\n",
      "Episode Reward: 0.0\n",
      "Step 221 (39947) @ Episode 163/10000, loss: 0.00047285296022892086\n",
      "Episode Reward: 1.0\n",
      "Step 52 (39999) @ Episode 164/10000, loss: 0.03540074825286865373\n",
      "Copied model parameters to target network.\n",
      "Step 207 (40154) @ Episode 164/10000, loss: 0.00059555424377322217\n",
      "Episode Reward: 1.0\n",
      "Step 175 (40329) @ Episode 165/10000, loss: 0.00095804629381746052\n",
      "Episode Reward: 0.0\n",
      "Step 195 (40524) @ Episode 166/10000, loss: 0.00094483641441911464\n",
      "Episode Reward: 1.0\n",
      "Step 371 (40895) @ Episode 167/10000, loss: 0.01862982101738453073\n",
      "Episode Reward: 3.0\n",
      "Step 247 (41142) @ Episode 168/10000, loss: 0.00073528604116290815\n",
      "Episode Reward: 2.0\n",
      "Step 185 (41327) @ Episode 169/10000, loss: 0.00158691057004034526\n",
      "Episode Reward: 0.0\n",
      "Step 234 (41561) @ Episode 170/10000, loss: 0.00068460736656561496\n",
      "Episode Reward: 1.0\n",
      "Step 316 (41877) @ Episode 171/10000, loss: 0.00039847404696047306\n",
      "Episode Reward: 3.0\n",
      "Step 326 (42203) @ Episode 172/10000, loss: 0.00056762492749840023\n",
      "Episode Reward: 3.0\n",
      "Step 234 (42437) @ Episode 173/10000, loss: 0.00050084391841664914\n",
      "Episode Reward: 1.0\n",
      "Step 265 (42702) @ Episode 174/10000, loss: 0.00146189518272876746\n",
      "Episode Reward: 2.0\n",
      "Step 281 (42983) @ Episode 175/10000, loss: 0.03597568348050117565\n",
      "Episode Reward: 2.0\n",
      "Step 233 (43216) @ Episode 176/10000, loss: 0.00050781015306711242\n",
      "Episode Reward: 1.0\n",
      "Step 307 (43523) @ Episode 177/10000, loss: 0.00109872559551149654\n",
      "Episode Reward: 2.0\n",
      "Step 173 (43696) @ Episode 178/10000, loss: 0.0015153326094150543\n",
      "Episode Reward: 0.0\n",
      "Step 246 (43942) @ Episode 179/10000, loss: 0.02707015536725521294\n",
      "Episode Reward: 1.0\n",
      "Step 173 (44115) @ Episode 180/10000, loss: 0.00056323758326470853\n",
      "Episode Reward: 0.0\n",
      "Step 167 (44282) @ Episode 181/10000, loss: 0.05642102658748627944\n",
      "Episode Reward: 0.0\n",
      "Step 245 (44527) @ Episode 182/10000, loss: 0.00098816142417490485\n",
      "Episode Reward: 1.0\n",
      "Step 255 (44782) @ Episode 183/10000, loss: 0.00086052145343273883\n",
      "Episode Reward: 2.0\n",
      "Step 242 (45024) @ Episode 184/10000, loss: 0.00051896681543439637\n",
      "Episode Reward: 2.0\n",
      "Step 208 (45232) @ Episode 185/10000, loss: 0.00079007213935256336\n",
      "Episode Reward: 1.0\n",
      "Step 134 (45366) @ Episode 186/10000, loss: 0.00087114039342850455"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-b9919f8f9a17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m                                     \u001b[0mepsilon_decay_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                                     \u001b[0mdiscount_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.99\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                                     batch_size=32):\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nEpisode Reward: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisode_rewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-51-9e453979c7a6>\u001b[0m in \u001b[0;36mdeep_q_learning\u001b[0;34m(sess, env, q_estimator, target_estimator, state_processor, num_episodes, experiment_dir, replay_memory_size, replay_memory_init_size, update_target_estimator_every, discount_factor, epsilon_start, epsilon_end, epsilon_decay_steps, batch_size, record_video_every)\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0mstates_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates_batch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 형변환 왜 또 하는거지..?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0;31m# Loss를 reduce_mean을 통해 scalar로 받아오고, Optimizing까지 해준다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-44-f4f440879496>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, sess, s, a, y)\u001b[0m\n\u001b[1;32m    108\u001b[0m         summaries, global_step, _, loss = sess.run(\n\u001b[1;32m    109\u001b[0m             \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummaries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_global_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m             feed_dict)\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary_writer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummaries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Where we save our checkpoints and graphs\n",
    "experiment_dir = os.path.abspath(\"./experiments/{}\".format(env.spec.id))\n",
    "\n",
    "# Create a glboal step variable\n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    \n",
    "# Create estimators\n",
    "q_estimator = Estimator(scope=\"q\", summaries_dir=experiment_dir)\n",
    "target_estimator = Estimator(scope=\"target_q\")\n",
    "\n",
    "# State processor\n",
    "state_processor = StateProcessor()\n",
    "\n",
    "# Run it!\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for t, stats in deep_q_learning(sess,\n",
    "                                    env,\n",
    "                                    q_estimator=q_estimator,\n",
    "                                    target_estimator=target_estimator,\n",
    "                                    state_processor=state_processor,\n",
    "                                    experiment_dir=experiment_dir,\n",
    "                                    num_episodes=10000,\n",
    "                                    replay_memory_size=500000,\n",
    "                                    replay_memory_init_size=50000,\n",
    "                                    update_target_estimator_every=10000,\n",
    "                                    epsilon_start=1.0,\n",
    "                                    epsilon_end=0.1,\n",
    "                                    epsilon_decay_steps=500000,\n",
    "                                    discount_factor=0.99,\n",
    "                                    batch_size=32):\n",
    "\n",
    "        print(\"\\nEpisode Reward: {}\".format(stats.episode_rewards[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
